[
{ "events": ["client:clientread"],
  "value": "Here are the key points regarding the Client:ClientRead wait event in Aurora PostgreSQL:\n- It occurs when Aurora PostgreSQL is waiting to receive data from the client before it can send more data back.\n- Likely causes include increased network latency, high load/saturation on the client side, excessive network round trips, large data copy operations, idle connections waiting for client input, and connection pooling settings.\n- Recommended actions:\n1. Place clients in the same AZ and VPC subnet as the cluster to reduce network latency.\n2. Scale up client resources (CPU, network) if they are constrained. \n3. Use current generation EC2 instances with jumbo frame support to reduce network round trips.\n4. Monitor and increase network bandwidth if saturated using CloudWatch metrics.\n5. For EC2 clients, use enhanced networking drivers and monitor network performance metrics.\n6. Check for and resolve increasing \"idle in transaction\" connections waiting on the client.\n7. If using PgBouncer, increase pkt_buf setting for large queries.\nEssentially, minimizing network latency, providing sufficient client-side resources, tuning connection pooling settings, and monitoring/resolving idle connections can help mitigate Client:ClientRead waits."
},
{
  "events": ["client:clientwrite"],
  "value": "Here are the key points about the Client:ClientWrite wait event in Aurora PostgreSQL:\n- It occurs when Aurora PostgreSQL is waiting to write data to the client because the client needs to read all previously sent data before more can be sent.\n- Likely causes include increased network latency, high CPU/network load on the client side causing delays in reading data, and large volumes of data being sent that overwhelm the client's ability to receive it quickly.\n- Recommended actions:\n1. Place clients in the same AZ and VPC subnet as the cluster to minimize network latency.\n2. Use current generation EC2 instances with jumbo frame support on the client side to increase network throughput.\n3. Modify application logic to reduce the amount of data needing to be sent to the client, if possible.\n4. Scale up client resources (CPU, network) if they are constrained and unable to keep up with receiving data.\n5. Monitor client CPU and network metrics using CloudWatch to identify bottlenecks.\nThe core issue is that the client is unable to read data from Aurora as fast as Aurora is sending it, leading to waits on the server side. Addressing network latency, client resource constraints, and reducing data volumes can help mitigate these waits."
},
{
  "events": ["cpu"],
  "value": "Here are the key points about the CPU wait event in Aurora PostgreSQL:\n- It indicates that a backend process is actively using or waiting for CPU time.\n- It can be identified when pg_stat_activity shows state='active' and wait_event_type/wait_event are null.\n- The DBLoadCPU metric in Performance Insights shows CPU usage by the database engine.\n- Likely causes of sudden CPU spikes include connection storms, workload changes like new queries/data size, changing execution plans, index maintenance etc.\n- Long-term high CPU could be due to too many concurrent backend processes, inefficient queries needing more buffers, CPU swapping/context switching.\n- Actions to take:\n1) Check if database or other processes are causing the CPU increase using Performance Insights metrics.\n2) Investigate if connection count has increased and take actions like connection pooling or upgrading instance size.\n3) Correlate CPU with metrics like block hits to identify inefficient queries and optimize them.\n4) For workload changes, revisit new queries, data size, indexing, parallel settings etc.\n5) Use Enhanced Monitoring to check for OS overload.\nIn essence, correlating CPU waits with other metrics helps identify if it is caused by database workload issues that can be tuned or whether other system/application factors are responsible."
},
{
  "events": ["io:buffileread", "io:buffilewrite"],
  "value": "Here are the key points about the IO:BufFileRead and IO:BufFileWrite wait events in Aurora PostgreSQL:\n- These occur when operations like sorts, joins, indexes etc. require more memory than allocated by work_mem or maintenance_work_mem parameters, causing temporary file spills to disk.\n- Common causes include:\n1) Queries needing more work memory (hash joins, order by, group by, window functions etc.)\n2) DDL operations like index creation, clustering needing more maintenance_work_mem\n3) Materialized view refreshes, CTAS statements involving memory-intensive operations\n- Actions to take:\n1) Identify problematic queries generating temp files using log_temp_files or cloudwatch metrics\n2) Optimize queries - check for cartesian joins, order/group by on too many columns\n3) Consider window functions instead of group by\n4) Use pg_repack for recreating large indexes\n5) Increase work_mem and/or maintenance_work_mem parameters\n6) Ensure enough memory is reserved for shared buffers\n7) Reduce connections via pooling if high #connections causing memory pressure\n- The goal is to allocate enough memory to avoid spilling while leaving room for other memory areas like shared buffers.\n- Monitoring metrics like FreeLocalStorage, temp file logs, wait events along with query analysis helps pinpoint and fix the root causes."
},
{
  "events": ["io:datafileread"],
  "value": "Here are the key points regarding the IO:DataFileRead wait event in Aurora PostgreSQL:\n- It occurs when a backend process needs to read a data page from storage because it is not available in the shared buffer pool.\n- Common causes include connection spikes, queries doing full sequential scans, CTAS/index creation on large tables, multiple vacuum workers, data ingestion triggering frequent ANALYZE, and resource starvation.\n- Recommended actions:\n1. Check predicates/filters in queries generating waits and consider indexing.\n2. Minimize impact of maintenance operations like VACUUM/ANALYZE by scheduling off-peak, partitioning large tables, disabling autoanalyze during large ingests.\n3. For high connection counts, use connection pooling, read replicas, scale up instance.\n4. Identify and optimize queries/operations doing full table scans using Performance Insights, pg_stat_statements etc.\n5. For large CTAS, index builds - increase maintenance_work_mem temporarily.\n6. Monitor for autovacuum triggers and tune vacuum parameters if needed.\n7. Ensure sufficient CPU, network bandwidth is available.\nThe core idea is to reduce the number of pages needing to be read from disk by effective indexing, partitioning, optimizing queries, allocating more memory and leveraging read replicas. Monitoring metrics and query analysis helps pinpoint the root causes to apply relevant tuning steps."
},
{
  "events": ["io:xactsync"],
  "value": "The IO:XactSync wait event in Aurora PostgreSQL occurs when the database is waiting for the storage subsystem to acknowledge transaction commits or rollbacks. Here are the key points:\nContext:\n- It indicates the instance is spending time waiting for storage to confirm transaction data was processed.\nLikely Causes:\n- Network saturation between clients/instance or instance/storage\n- CPU pressure preventing the Aurora storage daemon from getting enough CPU time\nActions:\n1. Monitor Resources:\n- Check WriteThroughput, CommitThroughput, WriteLatency, CommitLatency metrics\n- Check CPUUtilization - high values may starve the storage daemon\n2. Scale up CPU:\n- If CPU is the bottleneck, change to an instance type with more vCPUs\n3. Increase Network Bandwidth: \n- Check for other I/O related waits like IO:DataFileRead, Client:ClientRead etc.\n- If network is saturated, change to an instance type with higher network bandwidth\n4. Reduce Number of Commits:\n- Combine statements into transaction blocks to reduce commit overhead\nThe core issue is either insufficient CPU cycles for the storage daemon to process commits/rollbacks quickly, or network bottlenecks delaying the acknowledgment from storage. Monitoring metrics helps identify the constraint to address by scaling up CPU, network or reducing transactional workload."
},
{
  "events": ["ipc:damrecordtxack"],
  "value": "The IPC:DamRecordTxAck wait event in Aurora PostgreSQL is related to the Database Activity Streams (DAS) feature. Here are the key points:\nRelevant Versions: \n- All Aurora PostgreSQL 10.7 and higher 10.x versions\n- 11.4 and higher 11.x versions \n- All 12.x and 13.x versions\nContext:\n- It occurs when a session using DAS generates an activity stream event and waits for that event to become durable (in synchronous mode).\n- The session blocks other database activity while waiting, causing this wait event.\nCause:\n- Higher SQL activity generates more activity stream events that need durable writes, leading to IPC:DamRecordTxAck waits.\nActions:\n1) Reduce number of SQL statements or turn off DAS to reduce durable write events.\n2) Change DAS to asynchronous mode to reduce contention on this wait event.\n   - However, event durability is not guaranteed in async mode.\nIn essence, this wait is caused by the overhead of making each DAS audit event durable in synchronous mode. Reducing SQL activity, switching to async mode, or disabling DAS can help mitigate these waits if they impact performance significantly."
},
{
  "events": ["lock:advisory"],
  "value": "The Lock:advisory wait event in Aurora PostgreSQL occurs when an application uses advisory locks to coordinate activity across multiple sessions. Here are the key points:\nContext:\n- Advisory locks are application-level cooperative locks explicitly locked/unlocked by the application code.\n- They allow coordinating activity across sessions, unlike regular system locks.\n- Advisory locks can span transactions and are visible in pg_locks.\nCauses:\n- Queries acquiring advisory locks on more rows than returned, preventing unlocking.\n- Unintended lock ID conflicts between different application components.\n- Long-running sessions not releasing locks leading to buildup.\nActions:\n1. Review application usage of advisory locks - where they are acquired/released.\n2. Identify sessions acquiring too many locks or not releasing early using pg_stat_activity and pg_locks.\n3. Terminate long-running sessions causing lock buildup using pg_terminate_backend.\n4. Analyze lock information in pg_locks to find blocking/blocked sessions and queries.\n5. Check for unintended lock ID overlaps between application components.\nThe root causes are generally related to application logic issues in lock acquisition/release or conflicting lock ID usage across components. Monitoring pg_locks and pg_stat_activity helps identify the offending sessions and queries to tune the application's locking behavior accordingly."
},
{
  "events": ["lock:extend"],
  "value": "The Lock:extend wait event in Aurora PostgreSQL occurs when a backend process is waiting to lock a relation to extend it, while another process already has a lock on that relation for extension. Here are the key points:\nContext:\n- It indicates contention when multiple processes try to extend the same relation (table) concurrently during operations like INSERT, COPY, UPDATE.\n- Only one process can extend a relation at a time, so others wait, generating this event.\nLikely Causes:\n- Surge in concurrent inserts/updates to the same table\n- Insufficient network bandwidth causing storage latency\nActions:\n1. Identify tables with high insert/update contention using pg_stat_all_tables metrics.\n2. Check blocking/blocked queries in pg_stat_activity related to this wait event.\n3. Reduce contention:\n    - Use partitioning to separate inserts/updates across partitions\n    - Reduce fillfactor for update-heavy tables to minimize page extensions\n    - Test changes carefully as fillfactor impacts performance\n4. Check WriteLatency CloudWatch metric. If high, increase network bandwidth by scaling up instance size.\nThe core issue is multiple concurrent processes trying to extend the same table's storage, leading to lock contention. Separating the load via partitioning, tuning fillfactor, and ensuring sufficient network bandwidth can help mitigate these waits."
},
{
  "events": ["lock:relation"],
  "value": "The Lock:Relation wait event in Aurora PostgreSQL occurs when a query is waiting to acquire a lock on a table/view that is currently locked by another transaction. Here are the key points:\nContext:\n- Locks control concurrent access to tables/views (relations)\n- Incompatible lock modes like ACCESS EXCLUSIVE (DDL) and ROW EXCLUSIVE (DML) can block each other\n- Blocking can occur until the blocking query ends, transaction commits/rollbacks, or timeouts\nLikely Causes:\n- Increased concurrent sessions with conflicting table locks\n- Maintenance operations like VACUUM/ANALYZE acquiring exclusive locks\n- Locks on reader instances conflicting with writer locks (only ACCESS EXCLUSIVE replicated)\nActions:\n1. Reduce blocking SQL statements:\n   - Use NOWAIT option to cancel lock requests instead of waiting\n   - Set lock_timeout to limit wait time for locks\n2. Minimize maintenance operation impact:\n   - Schedule operations off-peak\n   - Tune autovacuum parameters\n3. Check reader locks:\n   - Query pg_locks, pg_stat_activity to identify blocking/blocked sessions\n   - Adjust max_standby_streaming_delay if reader locks timeout too quickly\nThe core issue is lock contention on tables/views due to conflicting lock modes requested by concurrent queries/DDL operations. Monitoring pg_locks and adjusting application logic or maintenance schedules can help reduce these waits."
},
{
  "events": ["lock:transactionid"],
  "value": "The Lock:transactionid wait event in Aurora PostgreSQL occurs when a transaction is waiting to acquire a row-level lock that is already held by another concurrent transaction. Here are the key points:\nContext:\n- It happens due to conflicting row-level locks between transactions involving UPDATE, SELECT FOR UPDATE, SELECT FOR KEY SHARE statements.\n- Blocked until the blocking transaction ends with COMMIT/ROLLBACK.\nLikely Causes:\n1. High Concurrency - More conflicts when highly concurrent workload contends for same rows.\n2. Idle in Transaction - Session has started a transaction but not committed/rolled back, holding row locks.\n3. Long-Running Transactions - Long-held row locks block other transactions.\nActions:\n1. For High Concurrency:\n   - Lower number of active sessions\n   - Use connection pooling\n   - Redesign app logic to avoid conflicting UPDATE/SELECT FOR UPDATE/KEY SHARE\n2. For Idle Transactions:\n   - Enable autocommit where possible\n   - Ensure all code paths end transactions explicitly \n   - Process query results after ending transactions\n3. For Long Transactions:\n   - Keep row locks out of long transactions\n   - Limit query length by using autocommit\nThe root cause is conflicting row-level locking requirements from concurrent transactions. Monitoring pg_stat_activity and pg_locks helps identify idle/long transactions. Reducing concurrency, enabling autocommit, and optimizing transaction/locking patterns in application code can mitigate these waits."
},
{
  "events": ["lock:tuple"],
  "value": "The Lock:tuple wait event in Aurora PostgreSQL occurs when a backend process is waiting to acquire a lock on a tuple (row) that is locked by another process. Here are the key points:\nContext:\n- It happens when multiple concurrent sessions try to update/delete the same row, causing lock conflicts.\n- One session waits for the other to commit/rollback and release the tuple lock.\nLikely Causes:\n- High number of concurrent UPDATE/DELETE statements on the same rows\n- Highly concurrent SELECT FOR UPDATE/NO KEY UPDATE statements \n- Connection pools opening more sessions executing the same operations\nActions:\n1. Investigate Application Logic:\n   - Check for long-running idle transactions holding tuple locks\n   - End blocker sessions (pg_terminate_backend) as short-term solution\n   - Use idle_in_transaction_session_timeout to end idle transactions automatically\n   - Leverage autocommit where possible\n2. Identify Blocker/Blocked Sessions:\n   - Query pg_stat_activity, pg_locks to find dependent locks\n   - Use aurora_stat_backend_waits function to analyze historical waits\n3. Reduce High Concurrency:\n   - Redesign tables/queries to reduce row-level contention\n   - Use NOWAIT clause with retry logic\n   - Consider optimistic/hybrid locking \n   - Change isolation levels\n4. Troubleshoot Bottlenecks:\n   - Scale up instance type\n   - Optimize resource-intensive queries\n   - Archive infrequently accessed data\nThe root cause is conflicting tuple-level locks due to highly concurrent updates/deletes on the same rows. Monitoring active locks, reducing concurrency, and tuning queries/transactions can help mitigate these waits."
},
{
  "events": ["lwlock:buffer_content"],
  "value": "The LWLock:buffer_content (or BufferContent in Aurora PostgreSQL 13+) wait event occurs when a session is waiting to read or write a data page in memory while another session has that page locked for writing. Here are the key points:\nContext:\n- PostgreSQL uses shared memory buffers to access data pages\n- Shared locks allow concurrent reads, exclusive locks block other locks\n- This event indicates contention when multiple processes try to lock the same buffer\nLikely Causes:\n- Increased concurrent updates to the same data (especially tables with many indexes)\n- Data for the active workload is not in memory, causing disk I/O delays\n- Excessive use of foreign key constraints holding buffer locks longer\nActions:\n1. Improve In-Memory Efficiency:\n   - Partition tables or scale up instance class to increase memory\n   - Ensure active working set fits in memory\n2. Reduce Foreign Key Constraints:\n   - Investigate workloads with high waits for unnecessary constraints\n   - Remove constraints if possible\n3. Remove Unused Indexes:\n   - Identify and drop indexes not being utilized\n   - Reduces locking overhead on data pages\nThe root cause is contention on buffer locks due to concurrent writes/updates on the same data pages, exacerbated when the working set does not fit in memory. Increasing memory, reducing constraints, removing unused indexes and efficient data partitioning can help mitigate these waits."
},
{
  "events": ["lwlock:buffer_mapping"],
  "value": "The LWLock:buffer_mapping (or LWLock:BufferMapping in Aurora PostgreSQL 13+) wait event occurs when a session is waiting to associate a data block with a buffer in the shared buffer pool. Here are the key points:\nContext:\n- It happens when processes need to load/remove pages from the shared buffer pool and acquire mapping locks.\n- The shared_buffers parameter sets the size of the shared buffer pool in memory.\nCauses:\n- Large queries requiring many pages to be loaded\n- Bloated indexes/tables leading to excessive page reads\n- Full table scans pulling in more data than fits in buffers\n- Shared buffer pool smaller than the working data set\nActions:\n1. Monitor buffer metrics:\n   - Watch BufferCacheHitRatio, blks_hit, blks_read metrics\n   - Declining hit ratio or increasing reads can precede mapping waits\n2. Assess indexing strategy: \n   - Check for index/table bloat causing extra page reads\n   - Ensure proper indexes exist for frequently used queries\n3. Reduce bursty buffer allocations:\n   - Perform smaller batch operations instead of large scans\n   - Partitioning tables can help reduce allocation bursts\nThe root cause is excessive data pages needing to be loaded/unloaded from the buffer pool, leading to mapping lock contention. Monitoring buffer metrics, optimizing indexes, rightsizing shared_buffers and reducing bursty allocations can help mitigate these waits."
},
{
  "events": ["lwlock:bufferio", "ipc:bufferio"],
  "value": "The LWLock:BufferIO (or IPC:BufferIO in Aurora PostgreSQL 14+) wait event occurs when PostgreSQL is waiting for other processes to finish I/O operations while concurrently trying to access the same page in the shared buffer pool. Here are the key points:\nContext:\n- Each shared buffer has an I/O lock associated with this wait event\n- It is used to handle multiple sessions requiring the same page to be read from storage into the buffer pool\n- The lock is released once the page is loaded into the shared buffers\nCauses:\n- Multiple backends trying to access the same page pending I/O\n- Mismatch between shared_buffers size and workload buffer requirements  \n- Frequent buffer evictions due to undersized shared_buffers\n- Bloated/missing indexes causing excessive page reads\n- Sudden spikes in connections accessing the same pages\nActions:\n1. Monitor BufferCacheHitRatio - declining values along with BufferIO waits may indicate undersized shared_buffers\n2. Tune max_wal_size, checkpoint_timeout based on workload peaks coinciding with BufferIO waits\n3. Remove unused indexes to reduce page reads\n4. Use partitioning to reduce index bloat impact\n5. Avoid unnecessary indexing \n6. Implement connection pooling to prevent connection spikes\n7. Restrict maximum connections as a best practice\nThe root cause is contention on the I/O locks when multiple backends concurrently need to load the same pages into the shared buffer pool. Proper sizing of shared_buffers, optimizing indexes, connection pooling and workload segregation can help mitigate these waits."
},
{
  "events": ["lwlock:lock_manager"],
  "value": "The LWLock:lock_manager wait event in Aurora PostgreSQL occurs when the engine needs to maintain the shared lock memory area to allocate, check, and deallocate locks when fast path locking is not possible. Here are the key points:\nContext:\n- Fast path locking reduces overhead for frequently acquired short-term locks unlikely to conflict\n- Non-fast path locking has higher overhead and can lead to this wait event\nLikely Causes:\n- Concurrent sessions exceeding vCPU count running queries that cannot use fast path locks\n- Heavily partitioned tables with queries not doing partition pruning\n- Connection storms creating excessive connections\n- DDL/DML locking frequently accessed relations\nActions:\n1. Use Partition Pruning to avoid scanning unnecessary partitions\n2. Remove unused indexes to reduce locking overhead\n3. Tune queries to use fast path locking where possible (< 16 relations)\n4. Address other high wait events like Lock:Relation that drive lock_manager waits\n5. Reduce hardware bottlenecks by scaling up instance type\n6. Implement connection pooling to prevent connection storms\n7. Upgrade to newer PostgreSQL versions with improved partition handling\nThe root cause is excessive non-fast path locking due to factors like lack of pruning, connection storms, unoptimized queries/schemas. Pruning, connection pooling, indexing strategy and upgrading the engine version can help mitigate these waits."
},
{
  "events": ["lwlock:multixactmemberbuffer", "lwlock:multixactoffsetbuffer", "lwlock:multixactmemberslru", "lwlock:multixactoffsetslru"],
  "value": "In Aurora PostgreSQL indicate that a session is waiting to retrieve a list of transactions that modify the same row in a table. Here are the key points:\nContext:\n- MultiXact is a data structure that stores a list of transaction IDs (XIDs) modifying the same table row\n- These events occur when retrieving the MultiXact data from memory/storage\nLikely Causes:\n- Sub-transactions from explicit savepoints or drivers/ORMs using automatic savepoints\n- Sub-transactions from PL/pgSQL exception handlers creating internal savepoints\n- Multiple transactions acquiring shared locks on parent rows due to foreign keys\nActions:\n1. Perform VACUUM FREEZE on affected tables/partitions for immediate relief\n2. Increase autovacuum frequency on tables exhibiting these waits\n   - Reduce autovacuum_multixact_freeze_max_age \n3. Increase memory parameters:\n   - multixact_offsets_cache_size\n   - multixact_members_cache_size\n4. Reduce long-running transactions that prevent vacuuming\n5. Long-term:\n   - Analyze DDL, remove unused indexes\n   - Reduce use of savepoints in transactions\n   - Check if foreign keys are needed\nThe root causes are excessive MultiXact data due to factors like sub-transactions, long transactions preventing vacuuming, and poor database design. Vacuuming, increasing memory parameters, reducing long transactions, and revisiting schema design can help mitigate these waits."
},
{
  "events": ["timeout:pgsleep"],
  "value": "The Timeout:PgSleep wait event in Aurora PostgreSQL occurs when a server process has called the pg_sleep function and is waiting for the specified sleep timeout to expire. Here are the key points:\nLikely Cause:\n- This event happens when an application, function or user issues a SQL statement that calls the pg_sleep, pg_sleep_for or pg_sleep_until functions.\n- These functions intentionally delay execution for the specified number of seconds.\nAction:\n- Identify the statement or code path that is calling the pg_sleep family of functions.\n- Determine if the intentional delay via these functions is appropriate or required in that context.\nUnless there is an explicit need to artificially delay execution in the application logic, the presence of this wait event generally indicates inefficient coding practices that should be revisited and optimized to avoid unnecessary blocking."
}
]